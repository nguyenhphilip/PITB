{
  "hash": "b462b706a3862018636a57bbce07dcb7",
  "result": {
    "markdown": "---\ntitle: \"Linear Regression\"\ndate: \"12-10-22\"\ncategories: [statistics, notes]\ntitle-block-banner: false\n---\n\n\n# Main\n\nLinear regression models the relationship between some outcome variable $Y_i$ based on a set of predictors $X_{ji}$ where $i$ indexes an individual case (e.g. a study subject) and $j \\in \\{1,2,..., p \\}$ given $p$ predictors. This yields the equation\n\n$$\nY_i = \\beta_0 + \\beta_1 X_{1i} + ... + \\beta_p X_{pi} + \\epsilon_i\n$$\n\nwhere $\\epsilon_i$ is an error term for a particular case $i$. The error terms model the variance left unexplained by the model and are called *residuals*. The goal is to estimate the beta terms in order to find a best fitting line.\n\nThe best fitting line is defined in many ways, but the most common involves minimizing the *least square criterion*. Minimizing the **least squares criterion** yields the beta values that minimize the residual sum of squares, i.e. $$\\text{RSS}=e_{1}^{2} + e_{2}^{2} + ... + e_{n}^{2}$$ where $e_i = y_i - \\hat{y}_i$ is the $i$th residual. Minimizing the residual sum of squares is equivalent to maximizing the variance that the model explains.\n\nIn the end what we have is the *estimated population regression line*, which predicts the average value of $Y$ given a particular value of $X$. For example, if we have a regression equation $${\\text{weight}} = \\beta_0 + \\beta_1 \\text{height}$$and we are given $\\beta_0, \\beta_1$, and $\\text{height}=50$ inches, we can plug those values in and get the **average weight** of a person who is 50 inches tall.\n\n# Assumptions\n\nMost of the assumptions revolve around the residuals:\n\n1. The residuals are normally distributed (*Normality assumption*) with a mean of $0$ and standard deviation of $\\sigma$ (*Homoscedasticity assumption*), i.e. $\\epsilon_i \\sim \\text{Normal}(0, \\sigma^2)$.\n2. The residuals should be independent, which is typically a function of good study design.\n3. *Linearity*: \n\n> The conditional mean of the errors is assumed to be zero for any given combination of values of the predictor variables. This implies that, for standard multiple regression models, the relationship between every independent variable $X_i$ and the population mean of the dependent variable $Y$, denoted by $\\mu_y$, is assumed to be linear when the other variables are held constant. Furthermore, the relations between the various $X_i$ and $\\mu_y$ are additive: thus, the relation of $X_i$ with $\\mu_y$ is the same, regardless of the value of $X_j$ ($j \\neq i$).\n\nThis states that the relationship between the various predictors $X_j$ and the *conditional mean* ($\\mathbb{E}[Y|X]$) is linear while the other variables are held constant, and additive. \n\n## Common Misconceptions re: assumptions\n\n#### X and Y don't have to be normally distributed\n\nFrom [Ernst and Albers 2017](http://dx.doi.org/10.7717/peerj.3323):\n\n> Most commonly, researchers incorrectly assume that X, or both X and Y, should be normally distributed, rather than the errors of the model.\n\nThey use the example of an independent [[t-test]]. Assume we are given two population distributions that are independent from each other (e.g. one distribution representing a treatment group, the other a control group), are normally distributed and have equal variance, and we want to compute a t-statistic. This can be done using a regression model where $X$ is a dummy variable that encodes each group, yielding a conditional mean for each group once the model is fit, $\\mathbb{E}[Y \\mid X]$. It's clear that $X$ is not normally distributed since it only takes on two values in this case:\n\n> This resulting 'conditional membership' distribution is nothing close to normal (as it takes on just two values), however no assumption of the general linear model is violated because the subpopulations of Y for each of the X values follow a normal distibution with equal variances.\n\n#### Non-linear relationships *can* be modeled using OLS regression\n\nThe linearity assumption only deals with linearity in the parameters and the estimates (e.g. $\\beta_j$), but not necessarily in the variables. For example, polynomial regression models have polynomial variables like $X$ and $X^{2}$.\n\n# Uses\n\nLinear regression can answer questions like:\n\n1. Is there an association between $X$ and $Y$?\n2. How strong is the relationship?\n3. If $X$ is multiclass, which class is associated with $Y$, and how large are the effects?\n4. How accurately can we predict $Y$ from $X$?\n5. Is the relationship between $X$ and $Y$ linear?\n6. Is there an interaction effect between $X$ and $Y$?\n\n# Example in R\n\nLet's create a correlated set of variables using simulation. We'll do that with `MASS:mvrnorm`, which minimally requires as inputs a vector $\\pmb{\\mu}$ that determines the mean of each variable, $\\Sigma$ which is a covariance matrix that describes how the two variables covary together, and $n$, the number of draws.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(corrr)\nset.seed(5)\nn <- 100\n# simulate standard normal\nmu <- c(0, 0)\n# Since the standard deviations of each distribution are 1\n# the covariance matrix is equivalent to the correlation matrix.\nsigma <- matrix(data = c(1, 0.15, \n                         0.15, 1), nrow = 2)\n\nd <- MASS::mvrnorm(n, mu, sigma) |>\n  as_tibble() |>\n  rename(x = `V1`,\n         y = `V2`)\n\ncorrelate(d) |>\n  shave() |>\n  rmarkdown::paged_table()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"x\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"y\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"x\",\"2\":\"NA\",\"3\":\"NA\"},{\"1\":\"y\",\"2\":\"0.05003705\",\"3\":\"NA\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nWe see that the correlation is $0.05$, even though we set it to be $0.15$. This is a result of the sample size $n=100$, which we can verify later. For now it is sufficient to model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd |>\n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\nFirst let's check our assumptions:\n\n1. Are the residuals independent?\n\nYes because we randomly generated the values. We know each draw is independent, and therefore so are the residuals.\n\n2. Are the residuals normally distributed around 0, and do they display homoskedasticity?\n\nLet's check homoskedasticity first. We can plot the fitted $y$ values against the residuals ($\\hat{y} - y$) to answer this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- lm(y ~ x, data = d)\ny_pred <- mod |> predict()\n\nd <- d |>\n  cbind(y_pred) |>\n  mutate(residuals = (y_pred - y))\n\nd |>\n  ggplot(aes(x = y_pred, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0) +\n  labs(title = \"Fitted values against residuals\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nThe residuals show no obvious pattern across $\\hat{y}$, which suggests that homoskedasticity holds.\n\nAs for normality, we can plot a QQplot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqplot(sample = y, data = d) +\n  labs(x = \"theoretical\", y = \"sample y\", title = \"QQ Plot\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nLooks good. \n\n3. Linearity\n\nAgain, from [Ernst and Albers 2017](http://dx.doi.org/10.7717/peerj.3323):\n\n> The linearity assumption can easily be checked using scatterplots or residual plots: plots of the residuals vs. either the predicted values of the dependent variable or against (one of) the independent variable(s).\n\nThey also mention that:\n\n> Residual plots are also the best visual check for homoscedasticity.\n\nWe did that above in (2), so we can safely say the linearity assumption holds.\n\nAll in all, we have met the ideal conditions to model our data using OLS linear regression. Let's look at the model summary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod |>\n  broom::tidy() |>\n  mutate(across(where(is.numeric), ~round(.x, 3))) |>\n  rmarkdown::paged_table()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"std.error\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"(Intercept)\",\"2\":\"0.034\",\"3\":\"0.105\",\"4\":\"0.323\",\"5\":\"0.747\"},{\"1\":\"x\",\"2\":\"0.056\",\"3\":\"0.114\",\"4\":\"0.496\",\"5\":\"0.621\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nSince our variables are standardized and we only have two variables in the model, the $\\beta$ estimate is equivalent to the correlation coefficient $r$. Here the estimate is $0.056$, which is pretty close to the correlation of the sample. But it isn't the actual value we used in the covariance matrix we used to generate our data!\n\nLet's increase the sample size and see what happens.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 100000\n# simulate standard normal\nmu <- c(0, 0)\n# Since the standard deviations of each distribution are 1\n# the covariance matrix is equivalent to the correlation matrix.\nsigma <- matrix(data = c(1, 0.15, \n                         0.15, 1), nrow = 2)\n\nd <- MASS::mvrnorm(n, mu, sigma) |>\n  as_tibble() |>\n  rename(x = `V1`,\n         y = `V2`)\n\ncorrelate(d) |>\n  shave() |>\n  fashion() |>\n  rmarkdown::paged_table()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"noquote\"],\"align\":[\"right\"]},{\"label\":[\"x\"],\"name\":[2],\"type\":[\"noquote\"],\"align\":[\"right\"]},{\"label\":[\"y\"],\"name\":[3],\"type\":[\"noquote\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"x\",\"2\":\"\",\"3\":\"\"},{\"1\":\"y\",\"2\":\".16\",\"3\":\"\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nThat looks much better. Let's verify it with `lm()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(y ~ x, data = d) |>\n  broom::tidy() |>\n  mutate(across(where(is.numeric), ~round(.x, 3))) |>\n  rmarkdown::paged_table()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"std.error\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"(Intercept)\",\"2\":\"-0.007\",\"3\":\"0.003\",\"4\":\"-2.261\",\"5\":\"0.024\"},{\"1\":\"x\",\"2\":\"0.157\",\"3\":\"0.003\",\"4\":\"50.182\",\"5\":\"0.000\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nMuch better. The beta estimate is closer to the parameter that generated the data.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}